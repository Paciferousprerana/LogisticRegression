{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "associate-sunset"
      },
      "source": [
        "# Credit risk modelling using Logistic Regression"
      ],
      "id": "associate-sunset"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "handled-tooth"
      },
      "source": [
        "## Problem Statement"
      ],
      "id": "handled-tooth"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accessory-watts"
      },
      "source": [
        "Predict the loan defaulters using a Logistic Regression model on the credit risk data and calculate credit scores"
      ],
      "id": "accessory-watts"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twenty-indonesia"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "twenty-indonesia"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "honest-friendship"
      },
      "source": [
        "* perform data exploration, preprocessing and visualization\n",
        "* implement Logistic Regression using manual code or using sklearn library\n",
        "* evaluate the model using appropriate performance metrics\n",
        "* develop a credit scoring system"
      ],
      "id": "honest-friendship"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lesbian-bottom"
      },
      "source": [
        "## Dataset"
      ],
      "id": "lesbian-bottom"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fixed-trainer"
      },
      "source": [
        "The dataset chosen for this project is the '**Give Me Some Credit**' dataset which can be used to build models for predicting loan repayment defaulters. This dataset contains 150000 data points and 11 features."
      ],
      "id": "fixed-trainer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caring-syndrome"
      },
      "source": [
        "### Download the dataset"
      ],
      "id": "caring-syndrome"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Packages"
      ],
      "metadata": {
        "id": "5nHbUGBPFdIC"
      },
      "id": "5nHbUGBPFdIC"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas==1.3.5"
      ],
      "metadata": {
        "id": "jc2WLiGnFg-p"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jc2WLiGnFg-p"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xverse"
      ],
      "metadata": {
        "id": "1mqj1J8TFjGS"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1mqj1J8TFjGS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appreciated-pattern"
      },
      "source": [
        "### Import Neccesary Packages"
      ],
      "id": "appreciated-pattern"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loose-marsh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from xverse.transformer import MonotonicBinning,WOE\n",
        "%matplotlib inline"
      ],
      "id": "loose-marsh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compressed-reflection"
      },
      "source": [
        "### Load the dataset"
      ],
      "id": "compressed-reflection"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fatty-graph"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "df = pd.read_csv('GiveMeSomeCredit.csv', index_col=0)\n",
        "df.head()"
      ],
      "id": "fatty-graph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "mijkceYI6yAP"
      },
      "id": "mijkceYI6yAP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experienced-sleeping"
      },
      "source": [
        "#### Describe the all statistical properties of the train dataset"
      ],
      "id": "experienced-sleeping"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greek-methodology"
      },
      "source": [
        "df.describe().T"
      ],
      "id": "greek-methodology",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "christian-hamilton"
      },
      "source": [
        "### Pre-processing"
      ],
      "id": "christian-hamilton"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "global-decision"
      },
      "source": [
        "#### Remove unwanted columns"
      ],
      "id": "global-decision"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pharmaceutical-latvia"
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(df.corr(), annot=True)"
      ],
      "id": "pharmaceutical-latvia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usual-elimination"
      },
      "source": [
        "#### Handle the missing data\n",
        "\n",
        "Find the how many null values in the dataset and fill with mean or remove."
      ],
      "id": "usual-elimination"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heated-findings"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "id": "heated-findings",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['MonthlyIncome'].fillna(df['MonthlyIncome'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "XYnlGkfr9lqr"
      },
      "id": "XYnlGkfr9lqr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['NumberOfDependents'].fillna(df['NumberOfDependents'].mean(), inplace=True)"
      ],
      "metadata": {
        "id": "bHwTnP1C-gFB"
      },
      "id": "bHwTnP1C-gFB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "7kpAfAcw-li_"
      },
      "id": "7kpAfAcw-li_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(df[df['age']<18].index, inplace=True)\n",
        "df[df['age']<18]"
      ],
      "metadata": {
        "id": "t47hf5OoBXob"
      },
      "id": "t47hf5OoBXob",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.boxplot(column='RevolvingUtilizationOfUnsecuredLines')"
      ],
      "metadata": {
        "id": "w68jW1hDCGax"
      },
      "id": "w68jW1hDCGax",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upper = np.percentile(df['RevolvingUtilizationOfUnsecuredLines'], 75)\n",
        "df['RevolvingUtilizationOfUnsecuredLines'] = df['RevolvingUtilizationOfUnsecuredLines'].apply(lambda x: upper if x>1 else x)"
      ],
      "metadata": {
        "id": "aN7WQFgZDVP3"
      },
      "id": "aN7WQFgZDVP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.boxplot(column='RevolvingUtilizationOfUnsecuredLines')"
      ],
      "metadata": {
        "id": "NmS-R3mbD0AT"
      },
      "id": "NmS-R3mbD0AT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hispanic-reply"
      },
      "source": [
        "### EDA &  Visualization"
      ],
      "id": "hispanic-reply"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "standing-cheese"
      },
      "source": [
        "#### Calculate the percentage of the target lebels and visualize with a graph"
      ],
      "id": "standing-cheese"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "attractive-hands"
      },
      "source": [
        "target0 = (df['SeriousDlqin2yrs'].value_counts()[0]*100)/len(df['SeriousDlqin2yrs'])\n",
        "target1 = (df['SeriousDlqin2yrs'].value_counts()[1]*100)/len(df['SeriousDlqin2yrs'])\n",
        "target_percent = [target0, target1]\n",
        "plt.bar(['0', '1'],target_percent)\n",
        "plt.title('percentages of target labels')"
      ],
      "id": "attractive-hands",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pie((df['SeriousDlqin2yrs'].value_counts()), labels = [0, 1], autopct = '%10.3f%%')\n",
        "plt.title(\"Percentages of customers with Serious deliquency in 2 years\")"
      ],
      "metadata": {
        "id": "09NTVTX4HaLs"
      },
      "id": "09NTVTX4HaLs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "satisfactory-stopping"
      },
      "source": [
        "#### Plot the distribution of SeriousDlqin2yrs by age"
      ],
      "id": "satisfactory-stopping"
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [df['age'].min(), 30, 60, 90, df['age'].max()]\n",
        "df['age_bins'] = pd.cut(x = df['age'], bins = bins, include_lowest = True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "RR7TatFvMelZ"
      },
      "id": "RR7TatFvMelZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "multiple-series"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "sns.countplot(data=df, x='age_bins', hue='SeriousDlqin2yrs', ax=ax)"
      ],
      "id": "multiple-series",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,5),dpi=110)\n",
        "plt.title(\"age vs SeriousDlqin2yrs\",fontsize=16)\n",
        "sns.regplot(data=df,y=\"age\",x=\"MonthlyIncome\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5eZLBfgKNuFU"
      },
      "id": "5eZLBfgKNuFU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "promotional-rolling"
      },
      "source": [
        "#### Calculate the correlation and plot the heatmap"
      ],
      "id": "promotional-rolling"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "studied-candidate"
      },
      "source": [
        "df[df.columns[:]].corr()\n",
        "sns.heatmap(df[train_data.columns[:]].corr(),fmt=\".1f\")\n",
        "plt.show()"
      ],
      "id": "studied-candidate",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operational-minute"
      },
      "source": [
        "### Data Engineering"
      ],
      "id": "operational-minute"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "outer-telephone"
      },
      "source": [
        "#### Weight of Evidence and Information value\n",
        "\n",
        "* Arrange the binning for each variable with different bins\n",
        "* Calculate information value and chooose the best features based on the rules given below\n",
        "\n",
        "| Information Value |\tVariable Predictiveness |\n",
        "| --- | --- |\n",
        "| Less than 0.02\t|  Not useful for prediction |\n",
        "| 0.02 to 0.1\t| Weak predictive Power |\n",
        "|  0.1 to 0.3 | Medium predictive Power |\n",
        "| 0.3 to 0.5 | Strong predictive Power |\n",
        "| >0.5 | Suspicious Predictive Power |\n",
        "\n",
        "* Calculate Weight of evidence for the selected variables"
      ],
      "id": "outer-telephone"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ordered-knock"
      },
      "source": [
        "X = df.drop(columns='SeriousDlqin2yrs', axis=1)\n",
        "y = df['SeriousDlqin2yrs']"
      ],
      "id": "ordered-knock",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xverse.transformer import MonotonicBinning\n",
        "\n",
        "clf = MonotonicBinning()\n",
        "clf.fit(X, y)\n",
        "\n",
        "print(clf.bins)\n",
        "output_bins = clf.bins"
      ],
      "metadata": {
        "id": "8BHczuI7gNXy"
      },
      "id": "8BHczuI7gNXy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MonotonicBinning(custom_binning=output_bins) #output_bins was created earlier\n",
        "\n",
        "out_X = clf.transform(X)\n",
        "out_X.head()"
      ],
      "metadata": {
        "id": "CWYC2gjCg0qf"
      },
      "id": "CWYC2gjCg0qf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xverse.transformer import WOE\n",
        "clf = WOE()\n",
        "clf.fit(X, y)\n",
        "clf.woe_df # weight of evidence transformation dataset. This dataset will be used in making bivariate charts as well. \n",
        "clf.iv_df #information value dataset"
      ],
      "metadata": {
        "id": "nvbIhgoghlkd"
      },
      "id": "nvbIhgoghlkd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.woe_df # weight of evidence transformation dataset. This dataset will be used in making bivariate charts as well. \n"
      ],
      "metadata": {
        "id": "ekAQtLUghxke"
      },
      "id": "ekAQtLUghxke",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_X_woe = clf.transform(X)"
      ],
      "metadata": {
        "id": "uquoJrnciyqP"
      },
      "id": "uquoJrnciyqP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_X_woe.head()"
      ],
      "metadata": {
        "id": "GDmv8anxjOfB"
      },
      "id": "GDmv8anxjOfB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conservative-rebel"
      },
      "source": [
        "### Identify features,  target and split it into train and test"
      ],
      "id": "conservative-rebel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ambient-dress"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "X = out_X_woe.drop(['SeriousDlqin2yrs'], axis=1)\n",
        "y = out_X_woe['SeriousDlqin2yrs']\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "id": "ambient-dress",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "decreased-sucking"
      },
      "source": [
        "### Logistic Regression from scratch using gradient method"
      ],
      "id": "decreased-sucking"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "precious-business"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return np.maximum(np.minimum(1 / (1 + np.exp(-x)), 0.9999), 0.0001)\n",
        "\n",
        "def cost_function(x, y, theta):\n",
        "  t = x.dot(theta)\n",
        "  return - np.sum(y * np.log(sigmoid(t)) + (1 - y) * np.log(1 - sigmoid(t))) / x.shape[0]\n",
        "\n",
        "def gradient_cost_function(x, y, theta):\n",
        "  t = x.dot(theta)\n",
        "  return x.T.dot(y - sigmoid(t)) / x.shape[0]\n",
        "\n",
        "def update_theta(x, y, theta, learning_rate):\n",
        "  return theta + learning_rate * gradient_cost_function(x, y, theta)\n",
        "\n",
        "def train(x, y, learning_rate, iterations=500, threshold=0.0005):\n",
        "  theta = np.zeros(x.shape[1])\n",
        "  costs = []\n",
        "  print('Start training')\n",
        "  for i in range(iterations):\n",
        "    theta = update_theta(x, y, theta, learning_rate)\n",
        "    cost = cost_function(x, y, theta)\n",
        "    print(f'[Training step #{i}] — Cost function: {cost:.4f}')\n",
        "    costs.append({'cost': cost, 'weights': theta})\n",
        "    if i > 15 and abs(costs[-2]['cost'] - costs[-1]['cost']) < threshold:\n",
        "      break\n",
        "  return theta, costs\n",
        "\n",
        "theta, costs = train(x_train, y_train, learning_rate=0.0001)\n",
        "\n",
        "def predict(x, theta):\n",
        "  return (sigmoid(x.dot(theta)) >= 0.5).astype(int)\n",
        "\n",
        "#Let’s compare, how predicted data are different than real:\n",
        "\n",
        "def get_accuracy(x, y, theta):\n",
        "  y_pred = predict(x, theta)\n",
        "  return (y_pred == y).sum() / y.shape[0]\n",
        "\n",
        "print(f'Accuracy on the training set: {get_accuracy(x_train, y_train, theta)}')\n",
        "\n",
        "print(f'Accuracy on the test set: {get_accuracy(x_test, y_test, theta)}')"
      ],
      "id": "precious-business",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reliable-black"
      },
      "source": [
        "### Implement the Logistic regression using sklearn\n",
        "\n",
        "As there is imbalance in the class distribution, add weightage to the Logistic regression.\n",
        "\n",
        "* Find the accuracy with class weightage in Logistic regression\n",
        "* Find the accuracy without class weightage in Logistic regression"
      ],
      "id": "reliable-black"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "impressive-assistant"
      },
      "source": [
        "# With weightage\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler().fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "X_test = scaler.transform(x_test)\n"
      ],
      "id": "impressive-assistant",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_lr = LogisticRegression(class_weight='balanced', random_state=123, max_iter=100)\n",
        "weighted_lr.fit(x_train, y_train)\n",
        "y_pred = weighted_lr.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "MJvR81lIrn3j"
      },
      "id": "MJvR81lIrn3j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "similar-flower"
      },
      "source": [
        "# Without weightage\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(x_train,y_train)\n",
        "y_pred = log_reg.predict(X_test)\n",
        "log_reg.score(X_test,y_test), log_reg.score(x_train, y_train)"
      ],
      "id": "similar-flower",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "signal-error"
      },
      "source": [
        "### Credit scoring\n",
        "\n",
        "When scaling the model into a scorecard, we will need both the Logistic Regression coefficients from model fitting as well as the transformed WoE values. We will also need to convert the score from the model from the log-odds unit to a points system.\n",
        "For each independent variable Xi, its corresponding score is:\n",
        "\n",
        "$Score = \\sum_{i=1}^{n} (-(β_i × WoE_i + \\frac{α}{n}) × Factor + \\frac{Offset}{n})$\n",
        "\n",
        "Where:\n",
        "\n",
        "βi — logistic regression coefficient for the variable Xi\n",
        "\n",
        "α — logistic regression intercept\n",
        "\n",
        "WoE — Weight of Evidence value for variable Xi\n",
        "\n",
        "n — number of independent variable Xi in the model\n",
        "\n",
        "Factor, Offset — known as scaling parameter\n",
        "\n",
        "  - Factor = pdo / ln(2); pdo is points to double the odds\n",
        "  - Offset = Round_of_Score - {Factor * ln(Odds)}"
      ],
      "id": "signal-error"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worst-spare"
      },
      "source": [
        "# Scaling factors\n",
        "coef = log_reg.coef_.ravel()\n",
        "intercept = log_reg.intercept_\n",
        "factor = 20/np.log(2)\n",
        "offset = 600 - ( factor * np.log(50))\n",
        "factor, offset"
      ],
      "id": "worst-spare",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwwnwQKMU_Nx"
      },
      "source": [
        "# 1st method\n",
        "# all_scores = []\n",
        "# for idx,row in X.iterrows():\n",
        "#   score  = []\n",
        "#   for j in range(len(row)):\n",
        "#     asum = (-((row[j] * coef[j]) + (intercept/X.shape[1])) * factor) + (offset/X.shape[1])\n",
        "#     score.append(asum)\n",
        "#   all_scores.append(sum(score))\n",
        "# max(all_scores), min(all_scores)"
      ],
      "id": "XwwnwQKMU_Nx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd method\n",
        "all_scores = []\n",
        "for idx,row in X.iterrows():\n",
        "  a = row.values * coef          # B_i * WOE_i\n",
        "  a = a + (intercept/X.shape[1]) # (B_i * WOE_i) + intercept_i / n\n",
        "  b = -a * factor                # -((B_i * WOE_i) + intercept_i / n) * factor\n",
        "  b = b + (offset/X.shape[1])    # -((B_i * WOE_i) + intercept_i / n) * factor) + offset / n\n",
        "  all_scores.append(sum(b))      # sum"
      ],
      "metadata": {
        "id": "4_P67nkLPtmv"
      },
      "id": "4_P67nkLPtmv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(all_scores),min(all_scores)"
      ],
      "metadata": {
        "id": "RGNluRTHPxT8"
      },
      "id": "RGNluRTHPxT8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(all_scores)"
      ],
      "metadata": {
        "id": "1zOhCb2VP0jx"
      },
      "id": "1zOhCb2VP0jx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intelligent-internship"
      },
      "source": [
        "### Performance Metrics"
      ],
      "id": "intelligent-internship"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "innocent-hygiene"
      },
      "source": [
        "#### Precision"
      ],
      "id": "innocent-hygiene"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "optimum-listening"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "precision_score(y_test, y_pred ,average='macro') "
      ],
      "id": "optimum-listening",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accessory-keyboard"
      },
      "source": [
        "#### Recall"
      ],
      "id": "accessory-keyboard"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "civic-corner"
      },
      "source": [
        "from sklearn.metrics import recall_score\n",
        "recall_score(y_test, y_pred,average='macro') "
      ],
      "id": "civic-corner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wired-amendment"
      },
      "source": [
        "#### Classification Report"
      ],
      "id": "wired-amendment"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "impossible-machinery"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "id": "impossible-machinery",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dense-feelings"
      },
      "source": [
        "#### Confusion matrix"
      ],
      "id": "dense-feelings"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "running-remains"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(y_test, y_pred)\n",
        "mat"
      ],
      "id": "running-remains",
      "execution_count": null,
      "outputs": []
    }
  ]
}